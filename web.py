import os
import pathlib
import uuid
import json
import pandas as pd
from werkzeug.utils import secure_filename
from azure.core.credentials import AzureKeyCredential
from flask import Flask, request, render_template, redirect, url_for
from dotenv import load_dotenv
import requests
from azure.storage.blob import BlobServiceClient
import azure.cognitiveservices.speech as speechsdk
from azure.ai.translation.text import TextTranslationClient, TranslatorCredential
from azure.ai.translation.text.models import InputTextItem
from azure.ai.vision.face import FaceAdministrationClient, FaceClient
from azure.ai.vision.face.models import FaceAttributeTypeRecognition04, FaceDetectionModel, FaceRecognitionModel, QualityForRecognition
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
from werkzeug.utils import secure_filename
from azure.ai.formrecognizer import DocumentAnalysisClient, DocumentAnalysisApiVersion
from azure.ai.formrecognizer import FormContentType
from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import TextCategory, AnalyzeTextOptions
from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes
from msrest.authentication import CognitiveServicesCredentials
from openai import AzureOpenAI  # âœ… ä½¿ç”¨æ–°ç‰ˆ openai å¥—ä»¶æ–¹å¼
from scipy import stats

# Load environment variables
load_dotenv(pathlib.Path(".env"))

app = Flask(__name__)
os.makedirs("static", exist_ok=True)
app.config['UPLOAD_FOLDER'] = 'static/uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# Azure service keys and endpoints
VISION_ENDPOINT = os.getenv("VISION_ENDPOINT")
VISION_KEY = os.getenv("VISION_KEY")
FACE_ENDPOINT = os.getenv("FACE_ENDPOINT")
FACE_KEY = os.getenv("FACE_KEY")
TRANSLATOR_KEY = os.getenv("TRANSLATOR_KEY")
TRANSLATOR_REGION = os.getenv("TRANSLATOR_REGION")
TRANSLATOR_ENDPOINT = os.getenv("TRANSLATOR_ENDPOINT")
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
TEXT_API_KEY = os.getenv("TEXT_API_KEY")
TEXT_API_ENDPOINT = os.getenv("TEXT_API_ENDPOINT")
DOC_KEY = os.getenv("DOC_INTELLIGENCE_KEY")
DOC_ENDPOINT = os.getenv("DOC_INTELLIGENCE_ENDPOINT")
CONTENT_SAFETY_KEY = os.getenv("CONTENT_SAFETY_KEY")
CONTENT_SAFETY_ENDPOINT = os.getenv("CONTENT_SAFETY_ENDPOINT")
AZURE_BLOB_CONN_STR = os.getenv("AZURE_BLOB_CONN_STR")
BLOB_CONTAINER = "photos"
AZURE_MAPS_KEY = os.getenv("AZURE_MAPS_KEY")
MAPS_CLIENT_ID = os.getenv("AZURE_MAPS_CLIENT_ID")
ANOMALY_KEY = os.getenv("ANOMALY_KEY")
ANOMALY_ENDPOINT = os.getenv("ANOMALY_ENDPOINT")

# Azure Blob client
blob_service = BlobServiceClient.from_connection_string(AZURE_BLOB_CONN_STR)
container_client = blob_service.get_container_client(BLOB_CONTAINER)
client = AzureOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    api_version="2023-12-01-preview",
    azure_endpoint=os.getenv("OPENAI_API_ENDPOINT"),
)
DEPLOYMENT_NAME = os.getenv("DEPLOYMENT_NAME")  # å¦‚ gpt4botã€chatbot ç­‰
@app.route("/")
def home():
    return render_template("home.html")

@app.route("/image_tool", methods=["GET", "POST"])
def index():
    image_url = ""
    description = ""
    tags = []
    tags_zh = []
    translation = ""
    blob_image_url = ""
    blob_audio_url = ""
    face_info = ""

    if request.method == "POST":
        image_url = request.form.get("image_url")
        pair_id = uuid.uuid4().hex

        # Computer Vision
        vision_url = VISION_ENDPOINT.rstrip("/") + "/vision/v3.2/analyze"
        headers = {"Ocp-Apim-Subscription-Key": VISION_KEY, "Content-Type": "application/json"}
        params = {"visualFeatures": "Description,Tags"}
        body = {"url": image_url}
        vision_resp = requests.post(vision_url, headers=headers, params=params, json=body)
        vision_data = vision_resp.json()
        description = vision_data.get("description", {}).get("captions", [{}])[0].get("text", "")
        tag_objs = vision_data.get("tags", [])
        if tag_objs:
            top_tag = sorted(tag_objs, key=lambda x: x.get("confidence", 0), reverse=True)[0]
            tags = [top_tag["name"]]

        # Translate description
        translator = TextTranslationClient(
            endpoint=TRANSLATOR_ENDPOINT,
            credential=TranslatorCredential(TRANSLATOR_KEY, TRANSLATOR_REGION)
        )
        translation_result = translator.translate(
            content=[InputTextItem(text=description)],
            from_parameter="en",
            to=["zh-Hant"]
        )
        translation = translation_result[0].translations[0].text

        # Translate tag
        if tags:
            tag_items = [InputTextItem(text=tag) for tag in tags]
            tag_result = translator.translate(content=tag_items, from_parameter="en", to=["zh-Hant"])
            tags_zh = [r.translations[0].text for r in tag_result]

        # Face API with debug
        try:
            face_url = FACE_ENDPOINT.rstrip("/") + "/face/v1.0/detect?returnFaceAttributes=age,gender,emotion"
            face_headers = {
                "Ocp-Apim-Subscription-Key": FACE_KEY,
                "Content-Type": "application/json"
            }
            face_params = {"returnFaceAttributes": "age,gender,emotion"}
            face_body = {"url": image_url}

            print("\nğŸ“¤ Face API request:", face_body)
            face_resp = requests.post(face_url, headers=face_headers, params=face_params, json=face_body)
            face_data = face_resp.json()
            print("ğŸ“¥ Face API response:", face_data)

            if isinstance(face_data, list) and len(face_data) > 0:
                face = face_data[0]
                age = face["faceAttributes"]["age"]
                gender = face["faceAttributes"]["gender"]
                emotion = max(face["faceAttributes"]["emotion"], key=face["faceAttributes"]["emotion"].get)
                face_info = f"å¹´é½¡ï¼š{age:.1f}ï¼Œæ€§åˆ¥ï¼š{gender}ï¼Œæƒ…ç·’ï¼š{emotion}"
            else:
                face_info = "æœªåµæ¸¬åˆ°è‡‰éƒ¨"
        except Exception as e:
            face_info = f"è‡‰éƒ¨åˆ†æå¤±æ•—ï¼š{str(e)}"

        # TTS
        speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
        audio_filename = f"speech_{pair_id}.mp3"
        audio_path = os.path.join("static", audio_filename)
        audio_output = speechsdk.audio.AudioOutputConfig(filename=audio_path)
        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)
        synthesizer.speak_text_async(translation).get()

        with open(audio_path, "rb") as audio_file:
            container_client.upload_blob(name=audio_filename, data=audio_file, overwrite=True)
        blob_audio_url = f"{container_client.url}/{audio_filename}"

        # Upload image
        image_resp = requests.get(image_url)
        image_filename = f"image_{pair_id}.jpg"
        container_client.upload_blob(name=image_filename, data=image_resp.content, overwrite=True)
        blob_image_url = f"{container_client.url}/{image_filename}"

        # Save metadata
        metadata = {
            "id": pair_id,
            "image": blob_image_url,
            "audio": blob_audio_url,
            "tags": tags,
            "tags_zh": tags_zh,
            "face": face_info
        }
        tagfile = f"meta_{pair_id}.json"
        container_client.upload_blob(name=tagfile, data=json.dumps(metadata), overwrite=True)

    return render_template("index.html",
                           image_url=blob_image_url,
                           description=description,
                           tags=tags,
                           tags_zh=tags_zh,
                           translation=translation,
                           audio_url=blob_audio_url,
                           face_info=face_info)

@app.route("/history")
def history():
    blobs = container_client.list_blobs()
    grouped = {}
    for blob in blobs:
        if blob.name.startswith("meta_") and blob.name.endswith(".json"):
            content = container_client.download_blob(blob).readall()
            data = json.loads(content)
            tags_zh = data.get("tags_zh", [])
            zh_tag = tags_zh[0] if tags_zh else "æœªçŸ¥"
            grouped.setdefault(zh_tag, []).append(data)
    return render_template("history.html", grouped=grouped)


@app.route("/delete/<item_id>", methods=["POST"])
def delete_item(item_id):
    try:
        container_client.delete_blob(f"image_{item_id}.jpg")
        container_client.delete_blob(f"speech_{item_id}.mp3")
        container_client.delete_blob(f"meta_{item_id}.json")
    except Exception as e:
        print("åˆªé™¤å¤±æ•—ï¼š", e)
    return redirect(url_for("history"))

@app.route("/go_history")
def go_history():
    return redirect(url_for("history"))

@app.route("/text_sentiment", methods=["GET", "POST"])
def text_sentiment():
    text_input = ""
    sentiment = ""
    confidence = ""
    warning_message = ""
    content_safety_results = {
        "hate": "",
        "self_harm": "",
        "sexual": "",
        "violence": ""
    }

    if request.method == "POST":
        text_input = request.form.get("user_text", "").strip()
        if not text_input:
            sentiment = "âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ–‡å­—"
            return render_template("sentiment.html", text=text_input, sentiment=sentiment, confidence="")

        # Sentiment analysis
        credential = AzureKeyCredential(TEXT_API_KEY)
        text_client = TextAnalyticsClient(endpoint=TEXT_API_ENDPOINT, credential=credential)
        documents = [{"id": "1", "text": text_input}]
        response = text_client.analyze_sentiment(documents=documents)[0]
        sentiment_map = {"positive": "æ­£é¢", "neutral": "ä¸­ç«‹", "negative": "è² é¢"}
        sentiment = sentiment_map.get(response.sentiment, response.sentiment)
        confidence = (
            f"æ­£é¢: {response.confidence_scores.positive:.2f}, "
            f"ä¸­ç«‹: {response.confidence_scores.neutral:.2f}, "
            f"è² é¢: {response.confidence_scores.negative:.2f}"
        )

        # Content safety analysis
        try:
            safety_client = ContentSafetyClient(CONTENT_SAFETY_ENDPOINT, AzureKeyCredential(CONTENT_SAFETY_KEY))
            safety_result = safety_client.analyze_text(AnalyzeTextOptions(text=text_input))
            SEVERITY_THRESHOLD = 2

            for item in safety_result.categories_analysis:
                if item.category == TextCategory.HATE:
                    content_safety_results["hate"] = f"ä»‡æ¨å…§å®¹åš´é‡ç¨‹åº¦ï¼š{item.severity}"
                    if item.severity >= SEVERITY_THRESHOLD:
                        warning_message = "âš ï¸ æ–‡å­—ä¸­å«æœ‰ä»‡æ¨èªè¨€ï¼Œè«‹å˜—è©¦è¼¸å…¥å…¶ä»–å…§å®¹ã€‚"
                elif item.category == TextCategory.SELF_HARM:
                    content_safety_results["self_harm"] = f"è‡ªæˆ‘å‚·å®³å…§å®¹åš´é‡ç¨‹åº¦ï¼š{item.severity}"
                    if item.severity >= SEVERITY_THRESHOLD:
                        warning_message = "âš ï¸ æ–‡å­—ä¸­å‡ºç¾è‡ªæˆ‘å‚·å®³å‚¾å‘ï¼Œè«‹é‡æ–°è¼¸å…¥å¥åº·å…§å®¹ã€‚"
                elif item.category == TextCategory.SEXUAL:
                    content_safety_results["sexual"] = f"æ€§å…§å®¹åš´é‡ç¨‹åº¦ï¼š{item.severity}"
                    if item.severity >= SEVERITY_THRESHOLD:
                        warning_message = "âš ï¸ æ–‡å­—ä¸­åŒ…å«æ•æ„Ÿæ€§å…§å®¹ï¼Œè«‹å˜—è©¦è¼¸å…¥å…¶ä»–å…§å®¹ã€‚"
                elif item.category == TextCategory.VIOLENCE:
                    content_safety_results["violence"] = f"æš´åŠ›å…§å®¹åš´é‡ç¨‹åº¦ï¼š{item.severity}"
                    if item.severity >= SEVERITY_THRESHOLD:
                        warning_message = "âš ï¸ æ–‡å­—ä¸­åŒ…å«æš´åŠ›ç›¸é—œå­—çœ¼ï¼Œè«‹å˜—è©¦è¼¸å…¥å…¶ä»–å…§å®¹ã€‚"
        except Exception as e:
            content_safety_results["error"] = f"âš ï¸ å®‰å…¨æ€§åˆ†æå¤±æ•—: {e}"

    return render_template(
        "sentiment.html",
        text=text_input,
        sentiment=sentiment,
        confidence=confidence,
        content_safety=content_safety_results,
        warning=warning_message
    )


doc_client = DocumentAnalysisClient(endpoint=DOC_ENDPOINT, credential=AzureKeyCredential(DOC_KEY))

@app.route("/pdf_summary", methods=["GET", "POST"])
def pdf_summary():
    summary = ""
    filename = ""

    if request.method == "POST":
        if "pdf_file" not in request.files:
            return render_template("pdf_summary.html", summary="æ²’æœ‰ä¸Šå‚³æª”æ¡ˆ")

        file = request.files["pdf_file"]
        if file.filename == "":
            return render_template("pdf_summary.html", summary="è«‹é¸æ“‡æª”æ¡ˆ")

        if file:
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(file_path)

            with open(file_path, "rb") as f:
                poller = doc_client.begin_analyze_document(
                    model_id="prebuilt-read",
                    document=f,
                    
                )

                result = poller.result()
                if result.pages:
                    text = "\n".join([line.content for page in result.pages for line in page.lines])
                else:
                    text = "âš ï¸ ç„¡æ³•æ“·å–å…§å®¹"

                summary = text[:1000] + "..." if len(text) > 1000 else text

    return render_template("pdf_summary.html", summary=summary, filename=filename)

vision_client = ComputerVisionClient(VISION_ENDPOINT, CognitiveServicesCredentials(VISION_KEY))

@app.route("/ocr_tool", methods=["GET", "POST"])
def ocr_tool():
    extracted_text = ""
    filename = ""

    if request.method == "POST":
        if "image_file" not in request.files:
            return render_template("ocr_tool.html", text="æ²’æœ‰ä¸Šå‚³æª”æ¡ˆ", filename="")

        file = request.files["image_file"]
        if file.filename == "":
            return render_template("ocr_tool.html", text="è«‹é¸æ“‡æª”æ¡ˆ", filename="")

        if file:
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(file_path)

            with open(file_path, "rb") as f:
                image_data = f.read()

            try:
                headers = {
                    "Ocp-Apim-Subscription-Key": VISION_KEY,
                    "Content-Type": "application/octet-stream"
                }
                params = {"api-version": "2023-10-01"}
                ocr_url = f"{VISION_ENDPOINT}/computervision/imageanalysis:analyze?features=read"
                response = requests.post(ocr_url, headers=headers, params=params, data=image_data)
                result = response.json()

                print("[DEBUG] OCR API å›å‚³ï¼š", json.dumps(result, indent=2, ensure_ascii=False))

                lines = []
                blocks = result.get("readResult", {}).get("blocks", [])
                for block in blocks:
                    for line in block.get("lines", []):
                        text = line.get("text", "")
                        if text:
                            lines.append(text)

                extracted_text = "\n".join(lines) if lines else "âš ï¸ æ²’æœ‰åµæ¸¬åˆ°æ–‡å­—"

            except Exception as e:
                extracted_text = f"âš ï¸ æ–‡å­—æ“·å–å¤±æ•—ï¼š{str(e)}"

    return render_template("ocr_tool.html", text=extracted_text, filename=filename)

# OCR route + Azure Maps integration
@app.route("/ocr_map_tool", methods=["GET", "POST"])
def ocr_map_tool():
    extracted_text = ""
    filename = ""
    map_coords = None
    query_address = ""

    print("ğŸ” [INFO] ä½¿ç”¨è€…é€²å…¥ /ocr_map_tool")

    if request.method == "POST":
        print("ğŸ“© [INFO] æ”¶åˆ° POST è«‹æ±‚")

        if "image_file" in request.files:
            file = request.files["image_file"]
            if file.filename != "":
                filename = secure_filename(file.filename)
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                file.save(file_path)
                print(f"ğŸ–¼ï¸ [INFO] ä¸Šå‚³åœ–ç‰‡å„²å­˜æˆåŠŸï¼š{file_path}")

                with open(file_path, "rb") as f:
                    image_data = f.read()

                try:
                    headers = {
                        "Ocp-Apim-Subscription-Key": VISION_KEY,
                        "Content-Type": "application/octet-stream"
                    }
                    params = {"api-version": "2023-10-01"}
                    ocr_url = f"{VISION_ENDPOINT}/computervision/imageanalysis:analyze?features=read"
                    response = requests.post(ocr_url, headers=headers, params=params, data=image_data)
                    result = response.json()

                    print("ğŸ“¤ [DEBUG] Azure OCR å›å‚³å…§å®¹ï¼š", result)

                    lines = []
                    read_result = result.get("readResult")
                    if read_result and "blocks" in read_result:
                        blocks = read_result["blocks"]
                        for block in blocks:
                            for line in block.get("lines", []):
                                lines.append(line.get("text", ""))
                        extracted_text = "\n".join(lines) if lines else "âš ï¸ æ²’æœ‰åµæ¸¬åˆ°æ–‡å­—"
                        print("ğŸ“„ [INFO] æ“·å–æ–‡å­—æˆåŠŸï¼š", extracted_text)
                    else:
                        extracted_text = f"âš ï¸ ç„¡æ³•æ“·å–æ–‡å­—ï¼š{result.get('error', {}).get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                        print("âš ï¸ [ERROR] OCR ç„¡æ³•æ“·å–æ–‡å­—")

                except Exception as e:
                    extracted_text = f"âš ï¸ æ–‡å­—æ“·å–å¤±æ•—ï¼š{str(e)}"
                    print("âŒ [EXCEPTION] OCR ç™¼ç”ŸéŒ¯èª¤ï¼š", e)

        if request.form.get("map_search"):
            query_address = request.form.get("address", "")
            print(f"ğŸ“ [INFO] ä½¿ç”¨è€…è¼¸å…¥åœ°å€ï¼š{query_address}")
            if query_address:
                try:
                    maps_url = "https://atlas.microsoft.com/search/address/json"
                    maps_params = {
                        "api-version": "1.0",
                        "subscription-key": AZURE_MAPS_KEY,
                        "query": query_address
                    }
                    maps_resp = requests.get(maps_url, params=maps_params)
                    maps_data = maps_resp.json()
                    print("ğŸ—ºï¸ [DEBUG] Azure Maps å›å‚³ï¼š", maps_data)

                    position = maps_data.get("results", [{}])[0].get("position", {})
                    if position:
                        map_coords = {
                            "lat": position.get("lat"),
                            "lon": position.get("lon")
                        }
                        print("âœ… [INFO] æŸ¥è©¢åº§æ¨™æˆåŠŸï¼š", map_coords)
                    else:
                        print("âš ï¸ [WARNING] æŸ¥ç„¡åº§æ¨™")

                except Exception as e:
                    extracted_text += f"\nâš ï¸ æŸ¥è©¢åœ°åœ–éŒ¯èª¤ï¼š{e}"
                    print("âŒ [EXCEPTION] æŸ¥è©¢åœ°åœ–éŒ¯èª¤ï¼š", e)

    print("ğŸ“¦ [DEBUG] å‚³å…¥æ¨¡æ¿çš„ map_coordsï¼š", map_coords)
    print("ğŸ”‘ [DEBUG] ä½¿ç”¨çš„ AZURE_MAPS_KEY æ˜¯å¦å­˜åœ¨ï¼š", bool(AZURE_MAPS_KEY))

    return render_template(
        "ocr_map_tool.html",
        text=extracted_text,
        filename=filename,
        map_coords=map_coords,
        query_address=query_address,
        azure_maps_key=AZURE_MAPS_KEY  # åŠ é€™è¡Œå¾ˆé—œéµï¼
    )

# ä¸»é ï¼šé£²é£Ÿæ¨è–¦
@app.route("/recommand", methods=["GET", "POST"])
def diet_recommend():
    result = ""
    if request.method == "POST":
        height = request.form.get("height")
        weight = request.form.get("weight")
        goal = request.form.get("goal")
        preference = request.form.get("preference")

        prompt = f"""
        ä½¿ç”¨è€…èº«é«˜ {height} å…¬åˆ†ï¼Œé«”é‡ {weight} å…¬æ–¤ï¼Œ
        é£²é£Ÿç›®æ¨™æ˜¯ã€Œ{goal}ã€ï¼Œé£²é£Ÿåå¥½æ˜¯ã€Œ{preference}ã€ã€‚
        è«‹ç”¨ç¹é«”ä¸­æ–‡æ¨è–¦ä»Šå¤©çš„ä¸‰é¤ï¼Œåˆ—å‡ºæ¯ä¸€é¤çš„å…§å®¹èˆ‡æ¨è–¦ç†ç”±ã€‚
        """

        try:
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            result = response.choices[0].message.content
        except Exception as e:
            result = f"â— ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

    return render_template("recommand.html", result=result)

@app.route("/anomaly_tool", methods=["GET", "POST"])
def anomaly_tool():
    result = ""
    filename = ""

    if request.method == "POST" and "csv_file" in request.files:
        file = request.files["csv_file"]
        if file.filename != "":
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config["UPLOAD_FOLDER"], filename)
            file.save(filepath)

            try:
                df = pd.read_csv(filepath)

                if "timestamp" not in df.columns or "value" not in df.columns:
                    result = "âŒ CSV æª”æ¡ˆä¸­éœ€åŒ…å« 'timestamp' å’Œ 'value' æ¬„ä½"
                else:
                    df["z_score"] = stats.zscore(df["value"])
                    df["isAnomaly"] = df["z_score"].abs() > 2  # z-score > 2 ç‚ºç•°å¸¸

                    output = []
                    for _, row in df.iterrows():
                        status = "ğŸ”´ ç•°å¸¸" if row["isAnomaly"] else "âœ… æ­£å¸¸"
                        output.append(f"{row['timestamp']} - {row['value']} - {status}")

                    result = "\n".join(output)

            except Exception as e:
                result = f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

    return render_template("anomaly_tool.html", result=result, filename=filename)



@app.route("/favicon.ico")
def favicon():
    return '', 204

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080, debug=True)